# Task ID: 10
# Title: Implement Model Structure Visualization
# Status: pending
# Dependencies: 3, 7
# Priority: low
# Description: Add tools to visualize the ViT model architecture.
# Details:
Develop functions to visualize the model's attention weights and layer connections. Use matplotlib or similar libraries to generate static visualizations.

# Test Strategy:
Generate visualizations for a trained model and verify they accurately represent the model structure.

# Subtasks:
## 1. Visualizing attention weights [done]
### Dependencies: None
### Description: Create visualizations to represent the attention weights of the model, showing how different parts of the input are weighted during processing.
### Details:
Implement code to extract attention weights from the model and generate heatmaps or other visual representations.
<info added on 2025-05-11T16:14:42.440Z>
Successfully implemented attention weight visualization functionality. Key achievements include:
1. Modified Vision Transformer model components to expose attention weights:
   - Added return_attention parameter to MultiHeadSelfAttention class
   - Enhanced TransformerEncoderBlock and TransformerEncoder to forward attention weights
   - Extended VisionTransformer's forward_features and forward methods to return attention weights

2. Created new attention_viz.py module with three visualization functions:
   - plot_attention_weights: Generates attention heatmaps for specific layers/heads
   - visualize_attention_on_image: Overlays attention weights on original images
   - visualize_all_heads: Displays all attention heads for a given layer

3. Updated __init__.py to export new visualization functions

4. Developed test script test_attention_viz.py with:
   - Support for random or user-provided images/models
   - Generation of all three visualization types
   - Flexible command-line configuration

Testing confirmed all functionality works, producing:
1. Attention heatmaps (attention_heatmap.png)
2. Image-overlaid attention (attention_on_image.png)
3. All-heads visualization (all_attention_heads.png)

Minor Chinese font warnings were observed but didn't affect functionality.
</info added on 2025-05-11T16:14:42.440Z>

## 2. Plotting layer connections [done]
### Dependencies: None
### Description: Generate diagrams or graphs that illustrate the connections between different layers of the model.
### Details:
Use tools like Graphviz or matplotlib to create visualizations of the model's architecture, showing how layers are interconnected.
<info added on 2025-05-12T02:51:00.652Z>
Successfully implemented Vision Transformer model layer connection visualization. Key achievements include: 1. Created a new `model_viz.py` module with three main visualization functions: `plot_model_structure` for overall model architecture, `plot_encoder_block` for detailed Transformer encoder block structure, and `visualize_layer_weights` for layer weight analysis. 2. Implemented two visualization methods: Graphviz for clear directed graphs (primary) and Matplotlib as a fallback. 3. Updated `src/visualization/__init__.py` for seamless integration. 4. Developed a comprehensive test script `scripts/test_model_viz.py` with CLI support for model type selection, output format customization, and default encoder block visualization. 5. Saved visualizations in `temp_metrics/plots` with Chinese labels for clarity.
</info added on 2025-05-12T02:51:00.652Z>

## 3. Generating static visualizations [pending]
### Dependencies: 10.1, 10.2
### Description: Produce static images or plots that summarize the model's structure and behavior for documentation or analysis.
### Details:
Combine the attention weight and layer connection visualizations into a cohesive set of static images, ensuring clarity and accuracy.

## 4. Testing with trained model [pending]
### Dependencies: 10.3
### Description: Validate the visualizations by applying them to a trained model and verifying their accuracy and usefulness.
### Details:
Run the visualization tools on a trained model instance, check for correctness, and refine the visualizations based on feedback.

